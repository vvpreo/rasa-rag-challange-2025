{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading '.gitattributes' to 'base_model/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete'\n",
      "Download complete. Moving file to base_model/.gitattributes\n",
      "Downloading 'README.md' to 'base_model/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.15be1d0739032f5907387b43c73353325a49cc4c.incomplete'\n",
      "Download complete. Moving file to base_model/README.md\n",
      "Downloading 'config.json' to 'base_model/.cache/huggingface/download/8_PA_wEVGiVa2goH2H4KQOQpvVY=.40e1f573d0d8a844df016c015a0dd4d38bfbba26.incomplete'\n",
      "Download complete. Moving file to base_model/config.json\n",
      "Downloading 'generation_config.json' to 'base_model/.cache/huggingface/download/3EVKVggOldJcKSsGjSdoUCN1AyQ=.3e22f6e907f99d57857ae62725aacfd251ca8e37.incomplete'\n",
      "Download complete. Moving file to base_model/generation_config.json\n",
      "Downloading 'model.safetensors' to 'base_model/.cache/huggingface/download/xGOKKLRSlIhH692hSVvI1-gpoa8=.5ac048c8614d6888b433a9ddb4ba8ae0633765757e99beb51e3edceeccdf3c4d.incomplete'\n",
      "Download complete. Moving file to base_model/model.safetensors\n",
      "Downloading 'special_tokens_map.json' to 'base_model/.cache/huggingface/download/ahkChHUJFxEmOdq5GDFEmerRzCY=.14761dcf1466dc232bd41de9c21d4c617b15755e.incomplete'\n",
      "Download complete. Moving file to base_model/special_tokens_map.json\n",
      "Downloading 'tokenizer.json' to 'base_model/.cache/huggingface/download/HgM_lKo9sdSCfRtVg7MMFS7EKqo=.1e9f59c515a01088e1f7f72b68ab99f92d15fb96.incomplete'\n",
      "Download complete. Moving file to base_model/tokenizer.json\n",
      "Downloading 'tokenizer.model' to 'base_model/.cache/huggingface/download/7iVfz3cUOMr-hyjiqqRDHEwVBAM=.dadfd56d766715c61d2ef780a525ab43b8e6da4de6865bda3d95fdef5e134055.incomplete'\n",
      "Download complete. Moving file to base_model/tokenizer.model\n",
      "Downloading 'tokenizer.model.v1' to 'base_model/.cache/huggingface/download/fVt5mJktonVAhgKu36DaecYhdJ0=.85c0803f3d614c4324dcc494a36cab796c77759f.incomplete'\n",
      "Download complete. Moving file to base_model/tokenizer.model.v1\n",
      "Downloading 'tokenizer_config.json' to 'base_model/.cache/huggingface/download/vzaExXFZNBay89bvlQv-ZcI6BTg=.4e3b15b7c812ae5d327523c5a1847f45882b4322.incomplete'\n",
      "Download complete. Moving file to base_model/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/rasa-rag-challange-2025/base_model\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "# download model\n",
    "huggingface-cli download \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\" \\\n",
    "    --token \"${HF_TOKEN}\" \\\n",
    "    --local-dir \"./base_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.3.19: Fast Mistral patching. Transformers: 4.50.3.\n",
      "   \\\\   /|    NVIDIA A100 80GB PCIe. Num GPUs = 1. Max memory: 79.254 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will load ./base_model as a legacy tokenizer.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "max_seq_length = 2048\n",
    "random_seed = 42\n",
    "\n",
    "\n",
    "# configure quantization method for base model\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# load quantized model and tokenizer from disk\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"./base_model\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.19 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# adapt model for peft\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=random_seed,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "from trl.extras.dataset_formatting import get_formatting_func_from_dataset\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "train = '/workspace/rasa-rag-challange-2025/tests/e2e_finetune/output_conversational/4_train_test_split/ft_splits/train.jsonl'\n",
    "eval_file = '/workspace/rasa-rag-challange-2025/tests/e2e_finetune/output_conversational/4_train_test_split/ft_splits/val.jsonl'\n",
    "\n",
    "# Load the training and evaluation datasets from JSONL files on disk\n",
    "train_dataset = datasets.load_dataset(\n",
    "    \"json\", data_files={\"train\": train}, split=\"train\"\n",
    ")\n",
    "eval_dataset = datasets.load_dataset(\n",
    "    \"json\", data_files={\"eval\": eval_file}, split=\"eval\"\n",
    ")\n",
    "\n",
    "# Uncomment the following line if you want to test prompt formatting on a single example from the eval dataset\n",
    "# print(get_formatting_func_from_dataset(train_dataset, tokenizer)(eval_dataset[0]))\n",
    "\n",
    "# Get a tokenizer with a chat template to format conversations according to a specified structure\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"llama-3\",  # Specifies the chat template format (options: zephyr, chatml, mistral, llama, alpaca, etc.)\n",
    "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},  # Maps dataset roles and messages to expected format\n",
    ")\n",
    "\n",
    "# Define a function to format prompts for each example in the dataset\n",
    "def formatting_prompts_func(examples):\n",
    "    # Extract conversation messages from each example\n",
    "    print([k for k in examples.keys()])\n",
    "    convos = examples[\"messages\"]\n",
    "    \n",
    "    # Apply the chat template to each conversation without tokenizing or adding generation prompts\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
    "    \n",
    "    # Return the formatted texts in a new dictionary key\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply the formatting function to both the training and evaluation datasets in batches\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
    "eval_dataset = eval_dataset.map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# configure training args\n",
    "args = TrainingArguments(\n",
    "    ###### training\n",
    "    seed = random_seed,\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    warmup_steps = 5,\n",
    "    #max_steps = 60,\n",
    "    num_train_epochs = 5,\n",
    "    learning_rate = 2e-4,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    ###### datatypes\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    ###### evaluation\n",
    "    eval_strategy = \"steps\",\n",
    "    eval_steps = 50,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    ###### outputs\n",
    "    logging_steps = 30,\n",
    "    output_dir = \"outputs\",\n",
    ")\n",
    "\n",
    "# setup trainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    max_seq_length = max_seq_length,\n",
    "    args = args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,826 | Num Epochs = 5 | Total steps = 1,140\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 41,943,040/3,794,014,208 (1.11% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1140' max='1140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1140/1140 2:00:14, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.470600</td>\n",
       "      <td>0.011620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.003033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.002687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.002575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.002591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.002611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.002704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.002588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.002581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.002610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.002637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.002592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.002662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.002611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.002657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.002724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.002592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.002632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.002675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but MistralForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    }
   ],
   "source": [
    "# run fine-tuning\n",
    "finetune_metrics = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "After fine-tuning, the base model and fine-tuned adapters are [merged together and saved to disk](https://docs.unsloth.ai/basics/saving-models/saving-to-vllm) in 16-bit for future compatibility with the [vLLM](https://github.com/vllm-project/vllm) model serving library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T15:53:20.009911Z",
     "start_time": "2025-03-31T15:53:19.925704Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 593.94 out of 944.44 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 70.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# save model to disk in 16-bit\n",
    "model.save_pretrained_merged(\"./finetuned_model_2\", tokenizer, save_method=\"merged_16bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot step against train and val losses\n",
    "log_history = pd.DataFrame(trainer.state.log_history)\n",
    "log_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-31T15:53:20.010524Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "eval_loss = log_history[[\"step\", \"loss\"]].dropna().plot(x=\"step\", ax=ax)\n",
    "train_loss = log_history[[\"step\", \"train_loss\"]].dropna().plot(x=\"step\", ax=ax)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "unsloth_env",
   "name": ".m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
